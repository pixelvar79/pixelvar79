### Sebastian Varela, PhD

ðŸ”¬ Researcher for the Center for Advanced Bioenergy and Biofuel Innovation, University of Illinois Urbana-Champaign 
ðŸ“§ sebavar79@gmail.com | ðŸ“š [Google Scholar](https://scholar.google.com/citations?hl=en&user=7VElQ60AAAAJ)

In a world increasingly dominated by data footprints, agriculture researchers can now use streams of data from diverse digital resources, ranging from underground proximal sensors to satellites, and pair 
them with field-level measurements to address questions that were seemingly impossible to answer only a short time ago. Research that integrates sensors, data-intensive analysis, and Artificial Intelligence (AI) 
while being driven by sound principles of plant and soil science are key to advancing agricultural science.

---

## Research Interests
- Artificial Intelligence in Agriculture
- Remote Sensing
- Sensor
- Automation, Apps Development in Agriculture

---

## Education
- Web Developer (2023-25)
- PhD in Agronomy and Remote Sensing, Kansas State University
- BSc in Soil and Crop Sciences, University of the Republic, Uruguay

---

## Publications

### Journal Articles

1. Varela, S.; Dhodda, P.R.; Hsu, W.H.; Prasad, P.V.V.; Assefa, Y.; Peralta, N.R.; Griffin, T.; Sharda, A.; Ferguson, A.; Ciampitti, I.A. (2017). *Spatio-temporal evaluation of plant height in corn via unmanned aerial systems*. Journal of Applied Remote Sensing, SPIE. [![DOI](https://img.shields.io/badge/DOI-10.1117/1.JRS.11.036013-blue)](https://doi.org/10.1117/1.JRS.11.036013) [![PDF](https://img.shields.io/badge/PDF-Download-orange)](papers/036013_1.pdf)

   
2. Varela, S.; Dhodda, P.R.; Hsu, W.H.; Prasad, P.V.V.; Assefa, Y.; Peralta, N.R.; Griffin, T.; Sharda, A.; Ferguson, A.; Ciampitti, I.A. (2018). *Early-Season Stand Count Determination in Corn via Integration of Imagery from Unmanned Aerial Systems (UAS) and Supervised Learning Techniques*. Remote Sensing. [![DOI](https://img.shields.io/badge/DOI-10.3390/rs10020343-blue)](https://doi.org/10.3390/rs10020343) [![PDF](https://img.shields.io/badge/PDF-Download-orange)](papers/remotesensing-10-00343-v2.pdf)

   
3. Varela, S.; Pederson, T.; Bernacchi, C.; Leakey, A. (2021). *Understanding Growth Dynamics and Yield Prediction of Sorghum Using High Temporal Resolution UAV Imagery Time Series and Machine Learning*. Remote Sensing. [![DOI](https://img.shields.io/badge/DOI-10.3390/rs13091763-blue)](https://doi.org/10.3390/rs13091763) [![PDF](https://img.shields.io/badge/PDF-Download-orange)](papers/remotesensing-13-01763-v2.pdf)


4. Varela, S.; Pederson, T.; Leakey, A. (2022). *Implementing Spatio-Temporal 3D-Convolution Neural Networks and UAV Time Series Imagery to Better Predict Lodging Damage in Sorghum*. Remote Sensing. [![DOI](https://img.shields.io/badge/DOI-10.3390/rs14030733-blue)](https://doi.org/10.3390/rs14030733) [![PDF](https://img.shields.io/badge/PDF-Download-orange)](papers/remotesensing-14-00733-v2.pdf)


5. Varela, S.; Zheng, X.; Njuguna, J.; Sacks, E.; Allen, D.; Ruhter, J.; Leakey, A.  (2022). *Deep Convolutional Neural Networks Exploit High-Spatial- and -Temporal-Resolution Aerial Imagery to Phenotype Key Traits in Miscanthus*. Remote Sensing. [![DOI](https://img.shields.io/badge/DOI-10.3390/rs14215333-blue)](https://doi.org/10.3390/rs14215333) [![PDF](https://img.shields.io/badge/PDF-Download-orange)](papers/remotesensing-14-05333.pdf)

    
### Recent Conference Talks
Invited speaker at the NAPPN (North American Plant Phenotyping Network) Annual Conference (2023). Danforth Center, St Louis, USA. Convolutional neural networks and generative and adversarial networks exploit high-spatial- and -temporal-resolution aerial imagery to phenotype key traits in Miscanthus. [![PDF](https://img.shields.io/badge/PDF-Download-orange)](papers/2023-NAPPN-abstract-SV.pdf)
<p align="center">
   <img src="papers/WhatsApp Image 2024-05-03 at 14.14.31.jpeg" width="300" height="200">

</p>
---

My passion for research is driven by a scientific curiosity on how large data silos can be turned into streams of biological insights. I'm interested in the in-depth investigations of two major 
topics: 

   1) novel AI to leverage scientific discovery,
   
   2) automation to reduce data silos and accelerate research cycles.


Direction 1 - Novel AI to leverage scientific discovery,

SIGNIFICANCE:

Disciplines such as biology, remote sensing, computer science, and advanced sensing technologies, are converging to tackle pressing challenges in agriculture. The increasing spatial, temporal, and spectral resolution of sensors, along with cloud computational processing, all enhance our ability to supply more detailed sensor data to models. Our capacity to manually supervise and automate (e.g., explicit parameterization, color thresholding, feature engineering, and selection of the best VIs, etc.) these processes is increasingly limiting.  

In addition, most advances in remote sensing of agriculture have relied on the fully supervised learning paradigm, which demands large amounts of labeled data for training. Acquiring such data can be costly and time-consuming, especially for complex tasks, and models trained under this paradigm may exhibit limited generalization, scalability, and domain adaptation. To address these challenges, computer science research is currently very active and prolific in topics such as self and semi-supervised learning, as well as new paradigms like 'zero-shot' learning. 

These advancements offer unprecedented opportunities to bridge gaps in the biology domain. However, achieving this requires permanent collaboration across disciplines, as well as rigorous testing to assess the transferability and adaptability of new techniques. My work addresses this technical and research need while maintaining a clear focus on the ultimate goal of accelerating agricultural research and the benefits it delivers to society.

During my time as a Postdoctoral Researcher and independent researcher at the Center for Advanced Bioenergy and Bioproducts Innovation (CABBI), I have developed a number of new capabilities for phenotyping genetically diverse populations of emerging bioenergy crops [3][4][5]. 

I started by investigating the growth dynamics of sorghum using a traditional remote sensing and feature extraction approaches to the analysis [1]. This was practically useful, but highlighted to me how manual supervision of some steps in the process was a bottleneck. To address these issues, I explored Convolutional Neural Networks (CNNs) [2][3] as an alternative approach that offered several advantages, such as minimizing the need for manual feature engineering and efficiently exploiting the temporal dimension of image time series data to analyze crop traits. The method supported both classification tasks (e.g. lodging detection) and regression problems (e.g. assessing lodging severity, flowering time, and yield prediction) in sorghum and miscanthus bioenergy crops, as shown in the left side figure extracted from one of the publications. The most significant and novel finding was that the efficacy of Unmanned Aerial Systems (UAS)-based remote sensing in rapidly and non-destructively assessing large-scale genetic variation in key traits was enhanced by utilizing a spatiotemporal CNN architecture compared to traditional time-point CNN architectures.	 

<p align="center">
   <img src="papers/paper1.jpg" width="400" height="400">
</p>

ON-GOING WORK:

More recently, I realized the importance of finding new solutions to the key bottleneck of deep learning models requiring large annotated datasets to learn from. My current work on this topic includes:
   1.	development of a multi-head CNN learning strategy for determining critical traits in miscanthus using aerial imagery. The goal here is to determine the level of transfer learning ability of the network between traits as a path to alleviate data collection on expensive traits versus more easily accessible ones.       This involves testing how much compensation in the predictive ability of the multi-head network occurs for each trait (i.e., yield, height, stem number and stem diameter of plants) when access to the ground-truth labels of each single trait is restricted but not for the other traits in the model. This project has been    achieved in partnership with an undergraduate in the UIUC double major program in Computer Science + Crop Science. I mentored him through winning a summer research fellowship form the American Society for Plant Biologists, completing his thesis research, and getting an internship at Corteva.

<p align="center">
   <img src="papers/Screenshot 2024-05-03 073833.png" width="400" height="400">
</p>

   2.	Integration of Autoencoders as a transfer learning strategy to assist semantic segmentation when assessing root traits from underground imagery. The goal here is to alleviate the need for manual annotation of roots in the images take from minirhizotrons, which is normally an essential but very laborious               requirement when training a segmentation model (example figure below). The initial hypothesis is that when an Autoencoder is being trained (i.e., unsupervised) for reconstructing root imageries, it should learn the salient features of the image. Those features can then be transferred to a segmentation model which        can then exploit them for training, and in doing so drastically cut the need for manual labeling of images for training purposes.   Preliminary results are encouraging, with the pretrained segmentation model being able perform equivalent while requiring 40% less manual training data. This corresponds to reducing the     need for human labelling by months.

<p align="center">
   <img src="papers/Picture2.jpg" width="400" height="400">
</p>

   3.	Development of a Generative and Adversarial learning strategy to break the bottleneck of labeled data to train an image classification tool, using assessment of flowering time in Miscanthus as a case study.  In the paper currently under review at PNAS, I demonstrated that the generative and adversarial learning       strategy allowed two orders of magnitude less training data to be used than for traditional fully supervised learning strategies without loss of accuracy. As a side effect, the GAN is progressively increasing its own ability to generate realistic representation of images as shown in the left side figure extracted        from the manuscript. A provisional patent application on this work is currently being prepared.
    
<p align="center">
   <img src="papers/Picture3.jpg" width="400" height="400">
</p>

FUTURE:

1.	I plan to continue accelerating the adoption of AI techniques that enhance generalization and reduce the need for manual supervision. In particular, I plan to test methods for the efficient integration of Autoencoders, Generative Adversarial Networks (GANs), Diffusion Models, and recent foundation models like the Segment Anything Model (SAM) and Large Language Models (LLMs) to drive solutions in agriculture research. This will include studying an end-to-end CNN learning strategy that directly learns target traits to simplify the scalability of the process. I anticipate that Diffusion Models and GAN strategies can more broadly aid in generating synthetic data for more efficient solutions to classification tasks (e.g., flowering detection, lodging detection, plant disease) and segmentation problems. So, building on my initial success in this area is a promising priority. Large Language Models (LLMs) are currently applied in domains such as healthcare, where they are fine-tuned for specific tasks and used as chat-bot assistants. I believe a compelling case for funding can be made to test parallel application of LLMs in agriculture. One priority will be to find suitable collaborators that can help me explore considerations such as data privacy and intellectual property as part of this process.
   
2.	Upon establishing my own research group, I would be very enthusiastic to explore novel modeling and sensor technologies that address current limitations of optical passive sensors, because this is currently a major limitation to crop traits assessments. The issues include: limited canopy penetration, shading, confounding factors, sensor angle and bidirectional reflectance distribution, and signal saturation. I believe there are opportunities to investigate the use of Autoencoders' compression ability to capture complex, nonlinear, and spatial relationships in large-dimensional hyperspectral imagery datasets. In parallel to this computational line of study, I will seek out partners to test emerging sensor technologies, such as active multispectral and hyperspectral LIDAR technologies.


- **Project Name:** Description of the project. [Link to Repository](https://github.com/pixelvar79/project)

- 
---

## Research Direction
- Open-source tools or datasets you have contributed to.

---

## Awards and Honors
- Fulbright Scholar (2014-16)

---

## Languages and Technologies
- Python
- Tensorflow
- AWS, Google Cloud Services
- Front and Back-end Developer

---

## Professional Memberships
- Association for the Advancement of Artificial Intelligence
- American Society for Photogrammetry and Remote Sensing
- American Society of Agronomy

---

## Fun Fact
- Love my dogs and with high expectations to be a good soccer player.
